#!/usr/bin/env python3
"""
HTMLè½¬Markdownç»Ÿä¸€å·¥å…·
æ”¯æŒå¾®ä¿¡å…¬ä¼—å·ã€çŸ¥ä¹ã€æ˜é‡‘ã€CSDNç­‰å¤šä¸ªå¹³å°
"""

import re
import sys
import argparse
import requests
from bs4 import BeautifulSoup
import html2text
from pathlib import Path
from urllib.parse import urlparse
import os


class PlatformDetector:
    """å¹³å°æ£€æµ‹å™¨"""

    @staticmethod
    def detect(url):
        """æ£€æµ‹URLæ‰€å±å¹³å°"""
        if 'mp.weixin.qq.com' in url:
            return 'wechat'
        elif 'zhihu.com' in url:
            return 'zhihu'
        elif 'xiaohongshu.com' in url or 'xhslink.com' in url:
            return 'xiaohongshu'
        elif 'juejin.cn' in url:
            return 'juejin'
        elif 'csdn.net' in url:
            return 'csdn'
        else:
            return 'generic'

    @staticmethod
    def get_platform_name(platform):
        """è·å–å¹³å°ä¸­æ–‡åç§°"""
        names = {
            'wechat': 'å¾®ä¿¡å…¬ä¼—å·',
            'zhihu': 'çŸ¥ä¹',
            'xiaohongshu': 'å°çº¢ä¹¦',
            'juejin': 'æ˜é‡‘',
            'csdn': 'CSDN',
            'generic': 'é€šç”¨ç½‘é¡µ'
        }
        return names.get(platform, 'æœªçŸ¥å¹³å°')


class BaseParser:
    """è§£æå™¨åŸºç±»"""

    def __init__(self):
        self.platform_name = 'æœªçŸ¥'

    def parse(self, soup):
        """è§£æé¡µé¢ï¼Œè¿”å›æ ‡é¢˜ã€ä½œè€…ã€å†…å®¹"""
        raise NotImplementedError

    def extract_media(self, content_tag):
        """æå–åª’ä½“èµ„æº"""
        media_list = []
        if not content_tag:
            return media_list

        # æå–å›¾ç‰‡
        for img in content_tag.find_all('img'):
            img_url = img.get('data-src') or img.get('src') or img.get('data-original')
            if img_url and img_url.startswith('http'):
                media_list.append({
                    'type': 'image',
                    'url': img_url,
                    'tag': img
                })

        # æå–è§†é¢‘
        for video in content_tag.find_all(['video', 'iframe']):
            video_url = video.get('data-src') or video.get('src')
            if video_url and video_url.startswith('http'):
                media_list.append({
                    'type': 'video',
                    'url': video_url,
                    'tag': video
                })

        return media_list


class WechatParser(BaseParser):
    """å¾®ä¿¡å…¬ä¼—å·è§£æå™¨"""

    def __init__(self):
        super().__init__()
        self.platform_name = 'å¾®ä¿¡å…¬ä¼—å·'

    def parse(self, soup):
        # æ£€æµ‹æ˜¯å¦æ˜¯è½®æ’­å›¾æ ¼å¼
        is_carousel = soup.find('div', class_='share_content_page') is not None

        if is_carousel:
            return self._parse_carousel(soup)
        else:
            return self._parse_regular(soup)

    def _parse_regular(self, soup):
        """è§£æå¸¸è§„æ–‡ç« """
        # æå–æ ‡é¢˜
        title = None
        title_tag = soup.find('h1', class_='rich_media_title')
        if title_tag:
            title = title_tag.get_text().strip()

        # æå–ä½œè€…
        author = None
        author_tag = soup.find('a', class_='rich_media_meta rich_media_meta_link rich_media_meta_nickname')
        if not author_tag:
            author_tag = soup.find('span', class_='rich_media_meta rich_media_meta_text')
        if author_tag:
            author = author_tag.get_text().strip()

        # æå–å‘å¸ƒæ—¶é—´
        publish_time = None
        time_tag = soup.find('em', id='publish_time')
        if time_tag:
            publish_time = time_tag.get_text().strip()

        # æå–æ­£æ–‡å†…å®¹
        content = soup.find('div', id='js_content')
        if not content:
            content = soup.find('div', class_='rich_media_content')

        if content:
            # ç§»é™¤ä¸å¿…è¦çš„æ ‡ç­¾
            for tag in content.find_all(['script', 'style']):
                tag.decompose()

        return {
            'title': title,
            'author': author,
            'publish_time': publish_time,
            'content': content
        }

    def _parse_carousel(self, soup):
        """è§£æè½®æ’­å›¾æ ¼å¼æ–‡ç« """
        import re
        import json
        from bs4 import BeautifulSoup as BS

        # æå–æ ‡é¢˜ï¼ˆä»JavaScriptå˜é‡ï¼‰
        title = None
        script_text = str(soup)
        title_match = re.search(r'window\.msg_title\s*=\s*[\'"]([^\'"]*)[\'"]', script_text)
        if title_match:
            title = title_match.group(1)

        # æå–ä½œè€…ï¼ˆä»é¡µé¢å…ƒç´ ï¼‰
        author = None
        author_tag = soup.find('div', class_='wx_follow_nickname')
        if author_tag:
            author = author_tag.get_text().strip()

        # è§£æè½®æ’­å›¾æ•°æ®
        picture_list_match = re.search(r'window\.picture_page_info_list\s*=\s*\[(.*?)\];', script_text, re.DOTALL)

        if not picture_list_match:
            print("è­¦å‘Š: æœªæ‰¾åˆ°è½®æ’­å›¾æ•°æ®")
            return {
                'title': title,
                'author': author,
                'publish_time': None,
                'content': None
            }

        # æå–æ‰€æœ‰å›¾ç‰‡URL
        cdn_urls = re.findall(r'cdn_url:\s*[\'"]([^\'"]+)[\'"]', picture_list_match.group(1))

        if not cdn_urls:
            print("è­¦å‘Š: è½®æ’­å›¾æ•°æ®ä¸­æœªæ‰¾åˆ°å›¾ç‰‡URL")
            return {
                'title': title,
                'author': author,
                'publish_time': None,
                'content': None
            }

        # åˆ›å»ºåŒ…å«æ‰€æœ‰è½®æ’­å›¾çš„HTMLå†…å®¹
        content_html = '<div class="carousel-content">\n'
        content_html += f'<h2>ğŸ“· å›¾ç‰‡è½®æ’­ ({len(cdn_urls)}å¼ )</h2>\n'

        for i, url in enumerate(cdn_urls, 1):
            # æ¸…ç†URLä¸­çš„è½¬ä¹‰å­—ç¬¦
            url = url.replace('\\x26amp;', '&').replace('\\x26', '&')
            content_html += f'<p><img src="{url}" alt="è½®æ’­å›¾ {i}" /></p>\n'

        content_html += '</div>'

        # è½¬æ¢ä¸ºBeautifulSoupå¯¹è±¡ä»¥ä¿æŒå…¼å®¹æ€§
        content = BS(content_html, 'html.parser')

        print(f"âœ“ æˆåŠŸè§£æè½®æ’­å›¾æ ¼å¼ï¼ŒåŒ…å« {len(cdn_urls)} å¼ å›¾ç‰‡")

        return {
            'title': title,
            'author': author,
            'publish_time': None,
            'content': content
        }


class ZhihuParser(BaseParser):
    """çŸ¥ä¹è§£æå™¨"""

    def __init__(self):
        super().__init__()
        self.platform_name = 'çŸ¥ä¹'

    def parse(self, soup):
        title = None
        title_tag = soup.find('h1', class_='Post-Title')
        if not title_tag:
            title_tag = soup.find('h1', class_='ArticleItem-title')
        if title_tag:
            title = title_tag.get_text().strip()

        author = None
        author_tag = soup.find('meta', {'name': 'author'})
        if author_tag:
            author = author_tag.get('content')

        content = soup.find('div', class_='RichContent-inner')
        if not content:
            content = soup.find('div', class_='Post-RichTextContainer')

        return {
            'title': title,
            'author': author,
            'publish_time': None,
            'content': content
        }


class JuejinParser(BaseParser):
    """æ˜é‡‘è§£æå™¨"""

    def __init__(self):
        super().__init__()
        self.platform_name = 'æ˜é‡‘'

    def parse(self, soup):
        title = None
        title_tag = soup.find('h1', class_='article-title')
        if title_tag:
            title = title_tag.get_text().strip()

        author = None
        author_tag = soup.find('span', class_='username')
        if author_tag:
            author = author_tag.get_text().strip()

        content = soup.find('article', class_='article-content')
        if not content:
            content = soup.find('div', class_='markdown-body')

        return {
            'title': title,
            'author': author,
            'publish_time': None,
            'content': content
        }


class CSDNParser(BaseParser):
    """CSDNè§£æå™¨"""

    def __init__(self):
        super().__init__()
        self.platform_name = 'CSDN'

    def parse(self, soup):
        title = None
        title_tag = soup.find('h1', class_='title-article')
        if title_tag:
            title = title_tag.get_text().strip()

        author = None
        author_tag = soup.find('a', class_='follow-nickName')
        if author_tag:
            author = author_tag.get_text().strip()

        content = soup.find('article', class_='baidu_pl')
        if not content:
            content = soup.find('div', id='article_content')

        return {
            'title': title,
            'author': author,
            'publish_time': None,
            'content': content
        }


class GenericParser(BaseParser):
    """é€šç”¨è§£æå™¨"""

    def __init__(self):
        super().__init__()
        self.platform_name = 'é€šç”¨ç½‘é¡µ'

    def parse(self, soup):
        # å°è¯•ä»å¸¸è§ä½ç½®æå–æ ‡é¢˜
        title = None
        for selector in ['h1', 'title', 'meta[property="og:title"]']:
            if selector.startswith('meta'):
                tag = soup.find('meta', {'property': 'og:title'})
                if tag:
                    title = tag.get('content')
            else:
                tag = soup.find(selector)
                if tag:
                    title = tag.get_text().strip()
            if title:
                break

        # å°è¯•æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
        content = None
        for selector in ['article', 'main', '.post-content', '.article-content', '.content']:
            if selector.startswith('.'):
                content = soup.find(class_=selector[1:])
            else:
                content = soup.find(selector)
            if content:
                break

        # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œå°è¯•æ‰¾æœ€å¤§çš„div
        if not content:
            all_divs = soup.find_all('div')
            if all_divs:
                content = max(all_divs, key=lambda x: len(x.get_text()))

        return {
            'title': title,
            'author': None,
            'publish_time': None,
            'content': content
        }


class HTML2Markdown:
    """HTMLè½¬Markdownä¸»ç±»"""

    def __init__(self, download_media=False):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'close',  # å¼ºåˆ¶å…³é—­keep-aliveï¼Œé¿å…HTTP/2é—®é¢˜
            'Referer': 'https://mp.weixin.qq.com/',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1',
            'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"macOS"',
        }
        self.download_media = download_media
        self.media_folder = None
        self.media_map = {}

        # åˆ›å»ºSessionå¹¶å¼ºåˆ¶ä½¿ç”¨HTTP/1.1ï¼ˆé¿å…HTTP/2 StreamReseté”™è¯¯ï¼‰
        self.session = requests.Session()

        # é…ç½®HTTPAdapterå¹¶ç¦ç”¨è¿æ¥æ± ï¼ˆå¼ºåˆ¶æ¯æ¬¡è¯·æ±‚æ–°å»ºè¿æ¥ï¼‰
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry

        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=5,
            backoff_factor=2,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"]
        )

        # åˆ›å»ºadapterï¼Œç¦ç”¨è¿æ¥æ± 
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=1,
            pool_maxsize=1,
            pool_block=False
        )

        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)

        # æ³¨å†Œæ‰€æœ‰è§£æå™¨
        self.parsers = {
            'wechat': WechatParser(),
            'zhihu': ZhihuParser(),
            'juejin': JuejinParser(),
            'csdn': CSDNParser(),
            'generic': GenericParser()
        }

    def _fetch_with_playwright(self, url):
        """ä½¿ç”¨Playwrightè·å–é¡µé¢ï¼ˆå¤„ç†åŠ¨æ€å†…å®¹å’ŒéªŒè¯ï¼‰"""
        try:
            from playwright.sync_api import sync_playwright
            print("æ­£åœ¨ä½¿ç”¨æµè§ˆå™¨æ¨¡å¼è·å–é¡µé¢...")

            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                context = browser.new_context(
                    user_agent=self.headers['User-Agent'],
                    viewport={'width': 1920, 'height': 1080},
                    locale='zh-CN'
                )
                page = context.new_page()

                # è®¾ç½®é¢å¤–çš„è¯·æ±‚å¤´
                page.set_extra_http_headers({
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Referer': 'https://mp.weixin.qq.com/'
                })

                # è®¿é—®é¡µé¢å¹¶ç­‰å¾…åŠ è½½å®Œæˆ
                page.goto(url, wait_until='networkidle', timeout=60000)

                # ç­‰å¾…ä¸€ä¸‹ç¡®ä¿åŠ¨æ€å†…å®¹åŠ è½½
                page.wait_for_timeout(2000)

                # è·å–é¡µé¢å†…å®¹
                content = page.content()
                browser.close()

                print("âœ“ æµè§ˆå™¨æ¨¡å¼è·å–æˆåŠŸ")
                return content

        except ImportError:
            print("è­¦å‘Š: Playwrightæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨æµè§ˆå™¨æ¨¡å¼")
            print("  å®‰è£…æ–¹æ³•: pip install playwright && playwright install chromium")
            return None
        except Exception as e:
            print(f"è­¦å‘Š: æµè§ˆå™¨æ¨¡å¼è·å–å¤±è´¥ - {e}")
            return None

    def _fetch_with_requests(self, url):
        """ä½¿ç”¨requestsè·å–é¡µé¢ï¼ˆå¿«é€Ÿè½»é‡ï¼‰"""
        max_retries = 5
        import time

        for attempt in range(max_retries):
            try:
                response = self.session.get(
                    url,
                    headers=self.headers,
                    timeout=30,
                    verify=True,
                    allow_redirects=True
                )
                response.raise_for_status()
                response.encoding = response.apparent_encoding or 'utf-8'
                return response.text
            except Exception as e:
                error_msg = str(e)
                # ç‰¹æ®Šå¤„ç†HTTP/2 StreamReseté”™è¯¯
                if 'StreamReset' in error_msg or 'stream_id' in error_msg:
                    wait_time = (attempt + 1) * 2
                    print(f"è­¦å‘Š: HTTP/2è¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                    if attempt < max_retries - 1:
                        print(f"  ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        # é‡æ–°åˆ›å»ºsessionï¼Œå¼ºåˆ¶ä½¿ç”¨HTTP/1.1
                        if attempt >= 2:
                            import requests
                            from requests.adapters import HTTPAdapter
                            from urllib3.util.retry import Retry

                            self.session = requests.Session()
                            retry_strategy = Retry(
                                total=3,
                                backoff_factor=1,
                                status_forcelist=[429, 500, 502, 503, 504],
                            )
                            adapter = HTTPAdapter(max_retries=retry_strategy)
                            self.session.mount("http://", adapter)
                            self.session.mount("https://", adapter)
                            print("  å·²åˆ‡æ¢åˆ° HTTP/1.1")
                        continue
                    else:
                        raise
                elif 'ConnectionError' in error_msg or 'timeout' in error_msg.lower():
                    wait_time = (attempt + 1) * 2
                    print(f"è­¦å‘Š: ç½‘ç»œé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                    if attempt < max_retries - 1:
                        print(f"  ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                    else:
                        raise
                else:
                    print(f"é”™è¯¯: æ— æ³•è·å–ç½‘é¡µå†…å®¹ - {e}")
                    raise

        return None

    def fetch_page(self, url):
        """
        æ™ºèƒ½è·å–ç½‘é¡µå†…å®¹ï¼ˆè‡ªåŠ¨fallbackåˆ°Playwrightï¼‰

        ç­–ç•¥:
        1. é»˜è®¤ä½¿ç”¨requestsï¼ˆå¿«é€Ÿè½»é‡ï¼‰
        2. å¦‚æœrequestså¤±è´¥ï¼Œè‡ªåŠ¨fallbackåˆ°Playwrightï¼ˆå¤„ç†éªŒè¯å’ŒåŠ¨æ€å†…å®¹ï¼‰
        3. å¯é€šè¿‡ç¯å¢ƒå˜é‡USE_PLAYWRIGHT=trueå¼ºåˆ¶ä½¿ç”¨Playwright
        """
        import os

        use_playwright = os.getenv('USE_PLAYWRIGHT', 'false').lower() == 'true'

        if use_playwright:
            # å¼ºåˆ¶ä½¿ç”¨Playwright
            print("å¼ºåˆ¶ä½¿ç”¨æµè§ˆå™¨æ¨¡å¼...")
            content = self._fetch_with_playwright(url)
            if content:
                return content
            raise Exception("Playwrightè·å–å¤±è´¥")
        else:
            # æ™ºèƒ½fallbackç­–ç•¥
            try:
                print("æ­£åœ¨ä½¿ç”¨å¿«é€Ÿæ¨¡å¼è·å–é¡µé¢...")
                content = self._fetch_with_requests(url)

                # æ£€æµ‹æ˜¯å¦æ˜¯éªŒè¯é¡µé¢
                if content and ('éªŒè¯' in content or 'æœªçŸ¥é”™è¯¯' in content) and len(content) < 5000:
                    print("æ£€æµ‹åˆ°å¯èƒ½çš„éªŒè¯é¡µé¢ï¼Œåˆ‡æ¢åˆ°æµè§ˆå™¨æ¨¡å¼...")
                    playwright_content = self._fetch_with_playwright(url)
                    if playwright_content:
                        return playwright_content
                    print("è­¦å‘Š: æµè§ˆå™¨æ¨¡å¼ä¹Ÿæ— æ³•è·å–ï¼Œä½¿ç”¨åŸå§‹å†…å®¹")

                return content
            except Exception as e:
                print(f"å¿«é€Ÿæ¨¡å¼å¤±è´¥: {e}")
                print("å°è¯•ä½¿ç”¨æµè§ˆå™¨æ¨¡å¼...")
                content = self._fetch_with_playwright(url)
                if content:
                    return content
                # ä¸¤ç§æ–¹å¼éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºåŸå§‹é”™è¯¯
                raise

    def download_file(self, url, save_path):
        """ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
        try:
            response = self.session.get(url, headers=self.headers, timeout=30, stream=True)
            response.raise_for_status()

            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            return True
        except Exception as e:
            print(f"  è­¦å‘Š: ä¸‹è½½å¤±è´¥ {url} - {e}")
            return False

    def download_media_files(self, media_list, base_name):
        """æ‰¹é‡ä¸‹è½½åª’ä½“æ–‡ä»¶"""
        if not media_list:
            return

        # åˆ›å»ºåª’ä½“æ–‡ä»¶å¤¹
        self.media_folder = f"{base_name}_files"
        Path(self.media_folder).mkdir(parents=True, exist_ok=True)

        # æå–æ–‡ä»¶å¤¹çš„basenameï¼ˆç”¨äºç›¸å¯¹è·¯å¾„ï¼‰
        media_folder_name = os.path.basename(base_name) + "_files"

        print(f"\nå¼€å§‹ä¸‹è½½åª’ä½“èµ„æº (å…± {len(media_list)} ä¸ª)...")

        downloaded = 0
        for idx, media in enumerate(media_list, 1):
            url = media['url']
            media_type = media['type']

            # ç”Ÿæˆæ–‡ä»¶å
            ext = self.get_file_extension(url, media_type)
            filename = f"{media_type}_{idx:03d}{ext}"
            save_path = os.path.join(self.media_folder, filename)

            # ä¸‹è½½æ–‡ä»¶
            print(f"  [{idx}/{len(media_list)}] ä¸‹è½½ {media_type}: {filename}")
            if self.download_file(url, save_path):
                # ä¿å­˜URLæ˜ å°„ï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼‰
                relative_path = os.path.join(media_folder_name, filename)
                self.media_map[url] = relative_path
                downloaded += 1
            else:
                # å¦‚æœä¸‹è½½å¤±è´¥ï¼Œä»ç„¶ä½¿ç”¨åŸå§‹URL
                self.media_map[url] = url

        print(f"âœ“ ä¸‹è½½å®Œæˆ: {downloaded}/{len(media_list)} ä¸ªæ–‡ä»¶")

    def get_file_extension(self, url, media_type):
        """è·å–æ–‡ä»¶æ‰©å±•å"""
        parsed = urlparse(url)
        path = parsed.path

        # å°è¯•ä»URLä¸­æå–æ‰©å±•å
        if '.' in path:
            ext = os.path.splitext(path)[1].split('?')[0]  # ç§»é™¤æŸ¥è¯¢å‚æ•°
            if ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.mp4', '.mov', '.avi']:
                return ext

        # æ ¹æ®ç±»å‹è¿”å›é»˜è®¤æ‰©å±•å
        if media_type == 'image':
            return '.jpg'
        elif media_type == 'video':
            return '.mp4'
        return ''

    def html_to_markdown(self, html_content, media_list):
        """å°†HTMLè½¬æ¢ä¸ºMarkdown"""
        if not html_content:
            return ""

        # å¤„ç†æ‰€æœ‰imgæ ‡ç­¾ï¼šå°†data-srcå¤åˆ¶åˆ°src
        for img in html_content.find_all('img'):
            if img.get('data-src') and not img.get('src'):
                img['src'] = img['data-src']
            elif img.get('data-original') and not img.get('src'):
                img['src'] = img['data-original']

        # å¤„ç†æ‰€æœ‰video/iframeæ ‡ç­¾
        for video in html_content.find_all(['video', 'iframe']):
            if video.get('data-src') and not video.get('src'):
                video['src'] = video['data-src']

        # å¦‚æœä¸‹è½½äº†åª’ä½“ï¼Œæ›¿æ¢HTMLä¸­çš„é“¾æ¥ä¸ºæœ¬åœ°è·¯å¾„
        if self.download_media and self.media_map:
            for media in media_list:
                url = media['url']
                tag = media['tag']
                if url in self.media_map:
                    local_path = self.media_map[url]
                    # æ›´æ–°æ ‡ç­¾çš„srcå±æ€§ä¸ºæœ¬åœ°è·¯å¾„
                    tag['src'] = local_path

        h = html2text.HTML2Text()
        h.ignore_links = False
        h.ignore_images = False
        h.ignore_emphasis = False
        h.body_width = 0
        h.unicode_snob = True
        h.skip_internal_links = True

        return h.handle(str(html_content))

    def clean_markdown(self, markdown_text):
        """æ¸…ç†Markdownæ–‡æœ¬"""
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        markdown_text = re.sub(r'\n{3,}', '\n\n', markdown_text)
        # ç§»é™¤é¦–å°¾ç©ºç™½
        markdown_text = markdown_text.strip()

        # ä¿®å¤åŒ…å«ç©ºæ ¼çš„å›¾ç‰‡å’Œè§†é¢‘é“¾æ¥
        def fix_image_path(match):
            alt_text = match.group(1)
            path = match.group(2)
            # å¦‚æœè·¯å¾„åŒ…å«ç©ºæ ¼ä¸”ä¸æ˜¯ä»¥ < å¼€å¤´ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
            if ' ' in path and not path.startswith('<'):
                return f'![{alt_text}](<{path}>)'
            return match.group(0)

        # åŒ¹é… ![alt](path) æ ¼å¼
        markdown_text = re.sub(r'!\[(.*?)\]\(([^)]+)\)', fix_image_path, markdown_text)

        return markdown_text

    def generate_filename(self, title, url):
        """ç”Ÿæˆæ–‡ä»¶å"""
        if title:
            # ç§»é™¤ä¸åˆæ³•çš„æ–‡ä»¶åå­—ç¬¦
            filename = re.sub(r'[<>:"/\\|?*]', '', title)
            filename = filename.strip()
            filename = filename[:100]  # é™åˆ¶é•¿åº¦
        else:
            # å¦‚æœæ²¡æœ‰æ ‡é¢˜ï¼Œä½¿ç”¨URLç”Ÿæˆ
            parsed = urlparse(url)
            filename = f"article_{parsed.path.split('/')[-1]}"

        return f"{filename}.md"

    def save_markdown(self, content, filepath):
        """ä¿å­˜Markdownæ–‡ä»¶"""
        try:
            Path(filepath).write_text(content, encoding='utf-8')
            print(f"âœ“ æ–‡ç« å·²ä¿å­˜åˆ°: {filepath}")
        except Exception as e:
            error_msg = f"ä¿å­˜æ–‡ä»¶å¤±è´¥ - {e}"
            print(f"é”™è¯¯: {error_msg}")
            raise Exception(error_msg)

    def convert(self, url, output_path=None, output_dir='output'):
        """ä¸»è½¬æ¢æµç¨‹"""
        print(f"æ­£åœ¨è·å–ç½‘é¡µ: {url}")

        # æ£€æµ‹å¹³å°
        platform = PlatformDetector.detect(url)
        platform_name = PlatformDetector.get_platform_name(platform)
        print(f"æ£€æµ‹åˆ°å¹³å°: {platform_name}")

        # è·å–ç½‘é¡µå†…å®¹
        html_content = self.fetch_page(url)
        if not html_content:
            error_msg = "æ— æ³•è·å–ç½‘é¡µå†…å®¹"
            print(f"é”™è¯¯: {error_msg}")
            raise Exception(error_msg)

        # è§£æé¡µé¢
        soup = BeautifulSoup(html_content, 'html.parser')
        parser = self.parsers.get(platform, self.parsers['generic'])
        article = parser.parse(soup)

        if not article['content']:
            error_msg = f"æœªèƒ½æ‰¾åˆ°æ–‡ç« å†…å®¹ - {platform_name}çš„é¡µé¢ç»“æ„å¯èƒ½å·²æ›´æ–°ï¼Œéœ€è¦è°ƒæ•´è§£æè§„åˆ™"
            print(f"è­¦å‘Š: {error_msg}")
            # ä¿å­˜åŸå§‹HTMLç”¨äºè°ƒè¯•
            debug_file = Path(output_dir) / 'debug.html'
            debug_file.parent.mkdir(exist_ok=True)
            debug_file.write_text(html_content, encoding='utf-8')
            print(f"å·²ä¿å­˜åŸå§‹HTMLåˆ°: {debug_file}ï¼ˆå¯ç”¨äºè°ƒè¯•ï¼‰")
            raise Exception(error_msg)

        print(f"âœ“ æˆåŠŸè§£ææ–‡ç« ")
        if article['title']:
            print(f"  æ ‡é¢˜: {article['title']}")
        if article['author']:
            print(f"  ä½œè€…: {article['author']}")

        # æå–åª’ä½“èµ„æº
        media_list = parser.extract_media(article['content'])
        if media_list:
            image_count = sum(1 for m in media_list if m['type'] == 'image')
            video_count = sum(1 for m in media_list if m['type'] == 'video')
            print(f"âœ“ æ‰¾åˆ° {len(media_list)} ä¸ªåª’ä½“èµ„æº (å›¾ç‰‡: {image_count}, è§†é¢‘: {video_count})")

        # ç¡®å®šè¾“å‡ºè·¯å¾„
        if not output_path:
            output_dir_path = Path(output_dir)
            output_dir_path.mkdir(exist_ok=True)
            output_path = output_dir_path / self.generate_filename(article['title'], url)
            output_path = str(output_path)
        else:
            Path(output_path).parent.mkdir(parents=True, exist_ok=True)

        # ä¸‹è½½åª’ä½“èµ„æºï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.download_media and media_list:
            base_name = os.path.splitext(output_path)[0]
            self.download_media_files(media_list, base_name)

        # æ„å»ºMarkdownå†…å®¹
        markdown_parts = []

        if article['title']:
            markdown_parts.append(f"# {article['title']}\n")

        metadata = []
        if article['author']:
            metadata.append(f"**ä½œè€…:** {article['author']}")
        if article.get('publish_time'):
            metadata.append(f"**å‘å¸ƒæ—¶é—´:** {article['publish_time']}")
        metadata.append(f"**æ¥æº:** {platform_name}")
        metadata.append(f"**åŸæ–‡é“¾æ¥:** {url}")

        if metadata:
            markdown_parts.append("\n".join(metadata))
            markdown_parts.append("\n---\n")

        # æ·»åŠ æ­£æ–‡
        content_md = self.html_to_markdown(article['content'], media_list)
        content_md = self.clean_markdown(content_md)
        markdown_parts.append(content_md)

        final_markdown = "\n".join(markdown_parts)

        # ä¿å­˜æ–‡ä»¶
        self.save_markdown(final_markdown, output_path)

        return output_path


def main():
    parser = argparse.ArgumentParser(
        description='HTMLè½¬Markdownç»Ÿä¸€å·¥å…· - æ”¯æŒå¾®ä¿¡å…¬ä¼—å·ã€çŸ¥ä¹ã€æ˜é‡‘ã€CSDNç­‰å¤šä¸ªå¹³å°',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ”¯æŒçš„å¹³å°:
  âœ… å¾®ä¿¡å…¬ä¼—å· (mp.weixin.qq.com)     - å®Œç¾æ”¯æŒ
  âœ… çŸ¥ä¹ (zhihu.com)                  - åŸºç¡€æ”¯æŒ
  âœ… æ˜é‡‘ (juejin.cn)                  - åŸºç¡€æ”¯æŒ
  âœ… CSDN (csdn.net)                   - åŸºç¡€æ”¯æŒ
  âš ï¸  å…¶ä»–ç½‘ç«™                          - é€šç”¨è§£æ

ä½¿ç”¨ç¤ºä¾‹:
  # å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼ˆæ¨èä½¿ç”¨ --download ä¸‹è½½å›¾ç‰‡ï¼‰
  %(prog)s https://mp.weixin.qq.com/s/xxxxx --download

  # çŸ¥ä¹æ–‡ç« 
  %(prog)s https://zhuanlan.zhihu.com/p/xxxxx

  # æ˜é‡‘æ–‡ç« 
  %(prog)s https://juejin.cn/post/xxxxx

  # æŒ‡å®šè¾“å‡ºç›®å½•
  %(prog)s https://xxxx --output-dir ./articles

  # æŒ‡å®šè¾“å‡ºæ–‡ä»¶
  %(prog)s https://xxxx -o my_article.md -d
        """
    )

    parser.add_argument('url', help='ç½‘é¡µURL')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¿å­˜åˆ°outputç›®å½•ï¼‰')
    parser.add_argument('-d', '--download', action='store_true',
                        help='ä¸‹è½½å›¾ç‰‡å’Œè§†é¢‘åˆ°æœ¬åœ°ï¼ˆé»˜è®¤åªä¿ç•™åœ¨çº¿é“¾æ¥ï¼‰')
    parser.add_argument('--output-dir', default='output',
                        help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: outputï¼‰')

    args = parser.parse_args()

    converter = HTML2Markdown(download_media=args.download)
    converter.convert(args.url, args.output, args.output_dir)


if __name__ == '__main__':
    main()
